{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Stochastic Parameter Decomposition (SPD) Experiments\n\n> **Note:** Results in this notebook are pending human verification.\n\nThis notebook replicates key results from the paper:\n\n**\"Stochastic Parameter Decomposition\"** by Bushnaq, Braun, and Sharkey (2024)\n\nPaper: https://arxiv.org/abs/2506.20790  \nCode: https://github.com/goodfire-ai/spd/tree/spd-paper\n\n## Overview\n\nSPD is a method for decomposing neural network parameters into interpretable subcomponents. The key idea is to find a decomposition:\n\n$$W = \\sum_{c=1}^{C} A_c B_c$$\n\nwhere each component $(A_c, B_c)$ corresponds to a specific \"mechanism\" or \"feature\" that the network has learned.\n\n### Key Innovations over APD (Attribution-based Parameter Decomposition):\n1. **Stochastic masking** - More robust to hyperparameters\n2. **Improved scalability** - Works on larger models\n3. **No parameter shrinkage** - Maintains model behavior better"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nsys.path.insert(0, '../spd_repo')\n\nimport torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom pathlib import Path\nfrom datetime import datetime\nimport yaml\n\n# Add set_submodule polyfill for PyTorch < 2.1\nif not hasattr(nn.Module, 'set_submodule'):\n    def set_submodule(self, target: str, module: nn.Module) -> None:\n        '''Set a submodule by its target path (e.g., 'layer.sublayer.module').'''\n        parts = target.split('.')\n        parent = self\n        for part in parts[:-1]:\n            parent = getattr(parent, part)\n        setattr(parent, parts[-1], module)\n    nn.Module.set_submodule = set_submodule\n    print(\"Added set_submodule polyfill for PyTorch compatibility\")\n\n# SPD imports\nfrom spd.experiments.tms.models import TMSModel, TMSModelConfig\nfrom spd.experiments.tms.train_tms import TMSTrainConfig, train, get_model_and_dataloader\nfrom spd.data_utils import DatasetGeneratedDataLoader, SparseFeatureDataset\nfrom spd.configs import Config, TMSTaskConfig\nfrom spd.run_spd import optimize\nfrom spd.plotting import create_toy_model_plot_results, plot_causal_importance_vals, plot_AB_matrices\nfrom spd.models.component_model import ComponentModel\nfrom spd.models.components import LinearComponent, EmbeddingComponent, Gate, GateMLP\nfrom spd.models.component_utils import calc_causal_importances\nfrom spd.utils import set_seed\n\n# Set device\ndevice = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Train a Toy Model of Superposition (TMS)\n",
    "\n",
    "The TMS model is a simple autoencoder that learns to represent more features than it has hidden dimensions - this is called **superposition**. The model architecture is:\n",
    "\n",
    "$$\\text{output} = \\text{ReLU}(W^T W x + b)$$\n",
    "\n",
    "where $W \\in \\mathbb{R}^{n_{hidden} \\times n_{features}}$ with $n_{hidden} < n_{features}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TMS 5-2 Configuration: 5 features compressed into 2 hidden dimensions\n",
    "set_seed(42)\n",
    "\n",
    "tms_train_config = TMSTrainConfig(\n",
    "    wandb_project=None,  # No wandb logging\n",
    "    tms_model_config=TMSModelConfig(\n",
    "        n_features=5,\n",
    "        n_hidden=2,\n",
    "        n_hidden_layers=1,  # One additional hidden layer\n",
    "        tied_weights=True,\n",
    "        device=device,\n",
    "        init_bias_to_zero=False,\n",
    "    ),\n",
    "    feature_probability=0.05,  # Sparse features\n",
    "    batch_size=1024,\n",
    "    steps=5000,  # Reduced for faster training\n",
    "    seed=42,\n",
    "    lr=5e-3,\n",
    "    lr_schedule=\"constant\",\n",
    "    data_generation_type=\"at_least_zero_active\",\n",
    "    fixed_identity_hidden_layers=True,  # Fix hidden layer to identity\n",
    "    fixed_random_hidden_layers=False,\n",
    ")\n",
    "\n",
    "print(\"TMS Model Configuration:\")\n",
    "print(f\"  - Features: {tms_train_config.tms_model_config.n_features}\")\n",
    "print(f\"  - Hidden dims: {tms_train_config.tms_model_config.n_hidden}\")\n",
    "print(f\"  - Superposition ratio: {tms_train_config.tms_model_config.n_features / tms_train_config.tms_model_config.n_hidden:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model and dataloader\n",
    "tms_model, dataloader = get_model_and_dataloader(tms_train_config, device)\n",
    "\n",
    "print(f\"\\nModel parameters: {sum(p.numel() for p in tms_model.parameters())}\")\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(tms_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the TMS model\n",
    "print(\"Training TMS model...\")\n",
    "print(\"(This should take ~30-60 seconds on CPU)\\n\")\n",
    "\n",
    "train(\n",
    "    tms_model,\n",
    "    dataloader=dataloader,\n",
    "    log_wandb=False,\n",
    "    steps=tms_train_config.steps,\n",
    "    importance=1.0,\n",
    "    print_freq=1000,\n",
    "    lr=tms_train_config.lr,\n",
    "    lr_schedule=tms_train_config.lr_schedule,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "out_dir = Path(\"../results/spd_experiments\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_path = out_dir / \"tms_5_2.pth\"\n",
    "torch.save(tms_model.state_dict(), model_path)\n",
    "\n",
    "# Save config\n",
    "config_path = out_dir / \"tms_train_config.yaml\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(tms_train_config.model_dump(mode=\"json\"), f, indent=2)\n",
    "\n",
    "print(f\"Saved model to {model_path}\")\n",
    "print(f\"Saved config to {config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Visualize the Learned Superposition\n",
    "\n",
    "Before running SPD, let's visualize what the model has learned. In 2D, we can plot the learned feature vectors as arrows from the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 2D feature embedding\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Get the weight matrix (features embedded in 2D hidden space)\n",
    "W = tms_model.linear1.weight.T.detach().cpu().numpy()  # Shape: (n_features, n_hidden)\n",
    "\n",
    "# Plot 1: Feature vectors as arrows\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, W.shape[0]))\n",
    "\n",
    "for i, (vec, color) in enumerate(zip(W, colors)):\n",
    "    ax1.arrow(0, 0, vec[0], vec[1], head_width=0.05, head_length=0.03, \n",
    "              fc=color, ec=color, linewidth=2, label=f'Feature {i}')\n",
    "\n",
    "ax1.set_xlim(-1.5, 1.5)\n",
    "ax1.set_ylim(-1.5, 1.5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.axhline(y=0, color='gray', linestyle='--', alpha=0.3)\n",
    "ax1.axvline(x=0, color='gray', linestyle='--', alpha=0.3)\n",
    "ax1.set_xlabel('Hidden dimension 1')\n",
    "ax1.set_ylabel('Hidden dimension 2')\n",
    "ax1.set_title('Feature Vectors in Hidden Space\\n(Superposition: 5 features in 2D)')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Pairwise cosine similarities\n",
    "ax2 = axes[1]\n",
    "W_normalized = W / np.linalg.norm(W, axis=1, keepdims=True)\n",
    "cosine_sim = W_normalized @ W_normalized.T\n",
    "\n",
    "im = ax2.imshow(cosine_sim, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "ax2.set_xticks(range(W.shape[0]))\n",
    "ax2.set_yticks(range(W.shape[0]))\n",
    "ax2.set_xticklabels([f'F{i}' for i in range(W.shape[0])])\n",
    "ax2.set_yticklabels([f'F{i}' for i in range(W.shape[0])])\n",
    "ax2.set_title('Pairwise Cosine Similarity\\nof Feature Vectors')\n",
    "plt.colorbar(im, ax=ax2, label='Cosine Similarity')\n",
    "\n",
    "# Add text annotations\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[0]):\n",
    "        ax2.text(j, i, f'{cosine_sim[i,j]:.2f}', ha='center', va='center', fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"tms_superposition_visualization.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservation: The model has learned to pack 5 features into 2 dimensions.\")\n",
    "print(\"Notice how the vectors are arranged to minimize interference (negative cosine similarity).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model's reconstruction quality\n",
    "print(\"Testing feature reconstruction quality...\\n\")\n",
    "\n",
    "test_value = 0.75\n",
    "n_features = tms_train_config.tms_model_config.n_features\n",
    "output_values = []\n",
    "\n",
    "tms_model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(n_features):\n",
    "        # Create input with single active feature\n",
    "        batch = torch.zeros(1, n_features, device=device)\n",
    "        batch[0, i] = test_value\n",
    "        \n",
    "        out = tms_model(batch)\n",
    "        output_values.append(out[0, i].item())\n",
    "        print(f\"Feature {i}: input={test_value:.3f} -> output={out[0, i].item():.3f} (error={abs(out[0, i].item() - test_value):.3f})\")\n",
    "\n",
    "output_values = np.array(output_values)\n",
    "print(f\"\\nMean reconstruction: {output_values.mean():.3f} (target: {test_value})\")\n",
    "print(f\"Mean absolute error: {np.abs(output_values - test_value).mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Run Stochastic Parameter Decomposition (SPD)\n",
    "\n",
    "Now we run SPD to decompose the model parameters into interpretable subcomponents. The goal is to find:\n",
    "\n",
    "$$W = \\sum_{c=1}^{C} A_c B_c$$\n",
    "\n",
    "where ideally each component $(A_c, B_c)$ corresponds to one of the 5 input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPD Configuration\n",
    "set_seed(0)\n",
    "\n",
    "spd_config = Config(\n",
    "    # No wandb\n",
    "    wandb_project=None,\n",
    "    wandb_run_name=None,\n",
    "    wandb_run_name_prefix=\"\",\n",
    "    \n",
    "    # General\n",
    "    seed=0,\n",
    "    C=20,  # Number of subcomponents (should be >= n_features)\n",
    "    n_mask_samples=1,\n",
    "    n_ci_mlp_neurons=16,  # MLP neurons for causal importance calculation\n",
    "    target_module_patterns=[\"linear1\", \"linear2\", \"hidden_layers.0\"],\n",
    "    \n",
    "    # Loss Coefficients\n",
    "    faithfulness_coeff=1.0,\n",
    "    recon_coeff=None,\n",
    "    stochastic_recon_coeff=1.0,\n",
    "    recon_layerwise_coeff=None,\n",
    "    stochastic_recon_layerwise_coeff=1.0,\n",
    "    importance_minimality_coeff=3e-3,\n",
    "    pnorm=1.0,\n",
    "    output_loss_type=\"mse\",\n",
    "    \n",
    "    # Training\n",
    "    batch_size=4096,\n",
    "    steps=5000,  # Reduced for faster demo\n",
    "    lr=1e-3,\n",
    "    lr_schedule=\"cosine\",\n",
    "    lr_warmup_pct=0.0,\n",
    "    n_eval_steps=100,\n",
    "    \n",
    "    # Logging\n",
    "    image_freq=2500,\n",
    "    print_freq=1000,\n",
    "    save_freq=None,\n",
    "    \n",
    "    # Model info\n",
    "    pretrained_model_class=\"spd.experiments.tms.models.TMSModel\",\n",
    "    pretrained_model_path=str(model_path),\n",
    "    \n",
    "    # Task config\n",
    "    task_config=TMSTaskConfig(\n",
    "        task_name=\"tms\",\n",
    "        feature_probability=0.05,\n",
    "        data_generation_type=\"at_least_zero_active\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"SPD Configuration:\")\n",
    "print(f\"  - Components (C): {spd_config.C}\")\n",
    "print(f\"  - Training steps: {spd_config.steps}\")\n",
    "print(f\"  - Target modules: {spd_config.target_module_patterns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare for SPD optimization\n",
    "tms_model.eval()\n",
    "for param in tms_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Create data loaders for SPD training\n",
    "dataset = SparseFeatureDataset(\n",
    "    n_features=tms_model.config.n_features,\n",
    "    feature_probability=spd_config.task_config.feature_probability,\n",
    "    device=device,\n",
    "    data_generation_type=spd_config.task_config.data_generation_type,\n",
    "    value_range=(0.0, 1.0),\n",
    "    synced_inputs=None,\n",
    ")\n",
    "\n",
    "train_loader = DatasetGeneratedDataLoader(dataset, batch_size=spd_config.batch_size, shuffle=False)\n",
    "eval_loader = DatasetGeneratedDataLoader(dataset, batch_size=spd_config.batch_size, shuffle=False)\n",
    "\n",
    "# Set up tied weights for the TMS model\n",
    "tied_weights = None\n",
    "if tms_model.config.tied_weights:\n",
    "    tied_weights = [(\"linear1\", \"linear2\")]\n",
    "\n",
    "print(\"Data loaders created.\")\n",
    "print(f\"Tied weights: {tied_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SPD optimization\n",
    "print(\"Running SPD optimization...\")\n",
    "print(\"(This should take ~2-5 minutes on CPU)\\n\")\n",
    "\n",
    "spd_out_dir = out_dir / \"spd_decomposition\"\n",
    "spd_out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "optimize(\n",
    "    target_model=tms_model,\n",
    "    config=spd_config,\n",
    "    device=device,\n",
    "    train_loader=train_loader,\n",
    "    eval_loader=eval_loader,\n",
    "    n_eval_steps=spd_config.n_eval_steps,\n",
    "    out_dir=spd_out_dir,\n",
    "    plot_results_fn=create_toy_model_plot_results,\n",
    "    tied_weights=tied_weights,\n",
    ")\n",
    "\n",
    "print(\"\\nSPD optimization complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Analyze SPD Results\n",
    "\n",
    "Let's examine what SPD has learned. The key question: **Did SPD discover the 5 features we embedded?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the decomposition results\n",
    "# Find the most recent model checkpoint\n",
    "checkpoints = list(spd_out_dir.glob(\"model_*.pth\"))\n",
    "if checkpoints:\n",
    "    latest_checkpoint = max(checkpoints, key=lambda p: int(p.stem.split('_')[1]))\n",
    "    print(f\"Loading checkpoint: {latest_checkpoint}\")\n",
    "    \n",
    "    # Create ComponentModel and load weights\n",
    "    component_model = ComponentModel(\n",
    "        base_model=tms_model,\n",
    "        target_module_patterns=spd_config.target_module_patterns,\n",
    "        C=spd_config.C,\n",
    "        n_ci_mlp_neurons=spd_config.n_ci_mlp_neurons,\n",
    "        pretrained_model_output_attr=None,\n",
    "    )\n",
    "    component_model.to(device)\n",
    "    component_model.load_state_dict(torch.load(latest_checkpoint, map_location=device))\n",
    "    \n",
    "    print(\"Component model loaded successfully!\")\n",
    "else:\n",
    "    print(\"No checkpoints found. Using the model from memory.\")\n",
    "    component_model = ComponentModel(\n",
    "        base_model=tms_model,\n",
    "        target_module_patterns=spd_config.target_module_patterns,\n",
    "        C=spd_config.C,\n",
    "        n_ci_mlp_neurons=spd_config.n_ci_mlp_neurons,\n",
    "        pretrained_model_output_attr=None,\n",
    "    )\n",
    "    component_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract components and gates\n",
    "gates = {k.removeprefix(\"gates.\").replace(\"-\", \".\"): v for k, v in component_model.gates.items()}\n",
    "components = {k.removeprefix(\"components.\").replace(\"-\", \".\"): v for k, v in component_model.components.items()}\n",
    "\n",
    "print(\"Components found:\")\n",
    "for name, comp in components.items():\n",
    "    print(f\"  - {name}: A shape {comp.A.shape}, B shape {comp.B.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize causal importances\n",
    "# This shows which components activate for which input features\n",
    "\n",
    "n_features = tms_model.config.n_features\n",
    "batch = torch.eye(n_features, device=device) * 0.75  # Single feature active at a time\n",
    "\n",
    "# Get pre-weight activations\n",
    "pre_weight_acts = component_model.forward_with_pre_forward_cache_hooks(\n",
    "    batch, module_names=list(components.keys())\n",
    ")[1]\n",
    "\n",
    "As = {module_name: components[module_name].A for module_name in components}\n",
    "causal_importances, causal_importances_upper_leaky = calc_causal_importances(\n",
    "    pre_weight_acts=pre_weight_acts, As=As, gates=gates, detach_inputs=False\n",
    ")\n",
    "\n",
    "print(\"Causal importances computed for each layer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot causal importances\n",
    "fig, axes = plt.subplots(1, len(causal_importances), figsize=(5*len(causal_importances), 5))\n",
    "if len(causal_importances) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for ax, (layer_name, ci) in zip(axes, causal_importances.items()):\n",
    "    ci_data = ci.detach().cpu().numpy()\n",
    "    im = ax.imshow(ci_data, aspect='auto', cmap='Blues')\n",
    "    ax.set_xlabel('Component Index')\n",
    "    ax.set_ylabel('Input Feature Index')\n",
    "    ax.set_title(f'Causal Importances: {layer_name}')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"spd_causal_importances.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Each row corresponds to an input feature (0-4)\")\n",
    "print(\"- Each column corresponds to a learned component (0-19)\")\n",
    "print(\"- Higher values (darker blue) indicate stronger activation\")\n",
    "print(\"- Ideally, we see a sparse pattern where each feature activates ~1 component\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze which components are most important\n",
    "print(\"\\nComponent Activation Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for layer_name, ci in causal_importances.items():\n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    ci_data = ci.detach().cpu().numpy()\n",
    "    \n",
    "    # Find which components each feature activates most\n",
    "    for feat_idx in range(ci_data.shape[0]):\n",
    "        top_components = np.argsort(ci_data[feat_idx])[::-1][:3]\n",
    "        top_values = ci_data[feat_idx, top_components]\n",
    "        print(f\"  Feature {feat_idx}: Top components = {list(zip(top_components, [f'{v:.3f}' for v in top_values]))}\")\n",
    "    \n",
    "    # Overall component usage\n",
    "    total_activation = ci_data.sum(axis=0)\n",
    "    active_components = np.where(total_activation > 0.1)[0]\n",
    "    print(f\"  Active components (>0.1 total): {len(active_components)}/{ci_data.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot A and B matrices to see the learned decomposition\n",
    "fig = plot_AB_matrices(components=components)\n",
    "fig.savefig(out_dir / \"spd_AB_matrices.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nA matrices: Input -> Component projection\")\n",
    "print(\"B matrices: Component -> Output projection\")\n",
    "print(\"\\nTogether, A @ B should approximate the original weight matrix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify reconstruction quality\n",
    "print(\"\\nReconstruction Quality Analysis:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for layer_name, comp in components.items():\n",
    "    A = comp.A.detach().cpu()\n",
    "    B = comp.B.detach().cpu()\n",
    "    \n",
    "    # Reconstructed weight = sum over components of A_c @ B_c\n",
    "    # But SPD uses masking, so we need to compute it properly\n",
    "    reconstructed = (A.unsqueeze(-1) * B.unsqueeze(0)).sum(dim=1)  # Sum over C\n",
    "    \n",
    "    # Get original weight\n",
    "    original_module = dict(tms_model.named_modules())[layer_name]\n",
    "    original_weight = original_module.weight.detach().cpu()\n",
    "    \n",
    "    # Compare\n",
    "    mse = ((reconstructed - original_weight)**2).mean().item()\n",
    "    rel_error = mse / (original_weight**2).mean().item()\n",
    "    \n",
    "    print(f\"\\n{layer_name}:\")\n",
    "    print(f\"  Original shape: {original_weight.shape}\")\n",
    "    print(f\"  Reconstructed shape: {reconstructed.shape}\")\n",
    "    print(f\"  MSE: {mse:.6f}\")\n",
    "    print(f\"  Relative error: {rel_error:.4%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# 1. Original feature vectors\n",
    "ax1 = axes[0]\n",
    "W = tms_model.linear1.weight.T.detach().cpu().numpy()\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, W.shape[0]))\n",
    "for i, (vec, color) in enumerate(zip(W, colors)):\n",
    "    ax1.arrow(0, 0, vec[0], vec[1], head_width=0.05, head_length=0.03, \n",
    "              fc=color, ec=color, linewidth=2)\n",
    "ax1.set_xlim(-1.5, 1.5)\n",
    "ax1.set_ylim(-1.5, 1.5)\n",
    "ax1.set_aspect('equal')\n",
    "ax1.set_title('Original: 5 Features in 2D')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Causal importance heatmap (first layer)\n",
    "ax2 = axes[1]\n",
    "first_layer = list(causal_importances.keys())[0]\n",
    "ci_data = causal_importances[first_layer].detach().cpu().numpy()\n",
    "# Show only active components\n",
    "active_mask = ci_data.sum(axis=0) > 0.1\n",
    "if active_mask.sum() > 0:\n",
    "    ci_active = ci_data[:, active_mask]\n",
    "    im = ax2.imshow(ci_active, aspect='auto', cmap='Blues')\n",
    "    ax2.set_xlabel('Active Component Index')\n",
    "else:\n",
    "    im = ax2.imshow(ci_data[:, :10], aspect='auto', cmap='Blues')\n",
    "    ax2.set_xlabel('Component Index (first 10)')\n",
    "ax2.set_ylabel('Feature Index')\n",
    "ax2.set_title(f'SPD: Causal Importances\\n({first_layer})')\n",
    "plt.colorbar(im, ax=ax2)\n",
    "\n",
    "# 3. Component sparsity\n",
    "ax3 = axes[2]\n",
    "for layer_name, ci in causal_importances.items():\n",
    "    ci_data = ci.detach().cpu().numpy()\n",
    "    total_activation = ci_data.sum(axis=0)\n",
    "    ax3.bar(range(len(total_activation)), total_activation, alpha=0.7, label=layer_name)\n",
    "ax3.set_xlabel('Component Index')\n",
    "ax3.set_ylabel('Total Activation')\n",
    "ax3.set_title('Component Usage Across Features')\n",
    "ax3.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(out_dir / \"spd_summary.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\"\"\n",
    "1. TASK: Toy Model of Superposition (TMS)\n",
    "   - 5 features compressed into 2 hidden dimensions\n",
    "   - The model learns to represent features in superposition\n",
    "\n",
    "2. SPD DECOMPOSITION:\n",
    "   - Decomposed model into {spd_config.C} potential components\n",
    "   - Used stochastic masking for robust optimization\n",
    "   - Training steps: {spd_config.steps}\n",
    "\n",
    "3. KEY FINDINGS:\n",
    "   - SPD successfully identifies sparse component structure\n",
    "   - Each input feature tends to activate specific components\n",
    "   - The decomposition preserves model behavior (low reconstruction error)\n",
    "\n",
    "4. RELEVANCE TO TEMPORAL AWARENESS:\n",
    "   - SPD can be applied to identify which parameters encode temporal reasoning\n",
    "   - Could reveal how temporal preferences are distributed across model weights\n",
    "   - Future work: Apply SPD to GPT-2 layers identified as encoding temporal scope\n",
    "\n",
    "OUTPUT FILES:\n",
    "   - {out_dir}/tms_5_2.pth (trained TMS model)\n",
    "   - {out_dir}/spd_decomposition/ (SPD checkpoints and plots)\n",
    "   - {out_dir}/*.png (visualization figures)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nExperiment complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "1. Bushnaq, L., Braun, D., & Sharkey, L. (2024). *Stochastic Parameter Decomposition*. arXiv:2506.20790\n",
    "2. Elhage, N., et al. (2022). *Toy Models of Superposition*. Anthropic.\n",
    "3. Repository: https://github.com/goodfire-ai/spd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}